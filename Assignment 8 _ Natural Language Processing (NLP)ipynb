{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNejx5Dax4BT2wMqDhFtwCT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Assignment 8 : Natural Language Processing (NLP)**\n","\n","# **1. Text Preprocessing:**\n","**• What is tokenization in NLP? Explain its importance.**\n","\n","`Tokenization` is the process of breaking down a sequence of text into smaller units, known as tokens. These tokens can be words, sentences, or subwords.\n","\n","**Importance of Tokenization:**\n","\n","1. **Foundation for Further Analysis:** Tokenization is the first step in most NLP tasks (such as text classification, sentiment analysis, machine translation, etc.), making it essential for enabling further text processing.\n","2. **Simplifies Text Representation:** By dividing the text into smaller units, tokenization allows each token to be analyzed for its meaning, function, or significance in a specific context.\n","3. **Text Preprocessing:** It allows for efficient filtering, stemming, lemmatization, and feature extraction, which are necessary for effective machine learning algorithms.\n","\n","\n"],"metadata":{"id":"UXHWanBGag8U"}},{"cell_type":"markdown","source":["**• Perform word and sentence tokenization on the followina text: \"Data science is an interdisciplinary field that uses scientific methods processes, algorithms, and systems to extract knowledge from data.\"**"],"metadata":{"id":"_L6W_YjuQSGT"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install nltk spacy transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ts_-LMArdGQV","executionInfo":{"status":"ok","timestamp":1738207497730,"user_tz":-345,"elapsed":4391,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"1f54895f-9b28-493e-bfed-24ba18f9e660"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"]}]},{"cell_type":"code","source":["# Download necessary resources for NLTK\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tImk1h7zPUFn","executionInfo":{"status":"ok","timestamp":1738207517640,"user_tz":-345,"elapsed":4767,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"b89fcc9f-3151-4f46-c244-1036c82b4148"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Download Spacy model\n","!python -m spacy download en_core_web_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C6tOAX8xPbyK","executionInfo":{"status":"ok","timestamp":1738207542056,"user_tz":-345,"elapsed":21801,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"f9905343-70a9-4ed5-d523-800600a7e2e6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize, sent_tokenize"],"metadata":{"id":"iAx3eHFZPi8u","executionInfo":{"status":"ok","timestamp":1738207545304,"user_tz":-345,"elapsed":384,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["text = \"Data science is an interdisciplinary field that uses scientific methods processes, algorithms, and systems to extract knowledge from data.\""],"metadata":{"id":"xeciwRPzPmYk","executionInfo":{"status":"ok","timestamp":1738207556259,"user_tz":-345,"elapsed":387,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Word Tokenization\n","word_tokens = word_tokenize(text)\n","print(\"Word Tokens:\", word_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_XZruqoPozO","executionInfo":{"status":"ok","timestamp":1738207566315,"user_tz":-345,"elapsed":410,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"3f829488-f4a5-4948-801e-83da5f62f4fc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Tokens: ['Data', 'science', 'is', 'an', 'interdisciplinary', 'field', 'that', 'uses', 'scientific', 'methods', 'processes', ',', 'algorithms', ',', 'and', 'systems', 'to', 'extract', 'knowledge', 'from', 'data', '.']\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPzIvwbiagPj","executionInfo":{"status":"ok","timestamp":1738207572998,"user_tz":-345,"elapsed":418,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"2aa83798-3703-47ee-dec0-8d72eed2cfe6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sentence Tokens: ['Data science is an interdisciplinary field that uses scientific methods processes, algorithms, and systems to extract knowledge from data.']\n"]}],"source":["# Sentence Tokenization\n","sentence_tokens = sent_tokenize(text)\n","print(\"\\nSentence Tokens:\", sentence_tokens)"]},{"cell_type":"markdown","source":["**• Explain stopwords in NLP. Why is it important to remove them?**\n","\n","`Stopword` are common words (such as \"the\", \"is\", \"in\", \"and\", \"or\", etc.) that are typically removed from text data during preprocessing because they do not carry significant meaning and do not contribute to the overall context of a sentence.\n","\n","**Importance of removing stopwords:**\n","\n","**Reduce Noise:** Removing stopwords helps focus on the more meaningful, content-carrying words in the text.\n","\n","**Improve Efficiency:** Processing large datasets becomes more efficient when stopwords are eliminated, as fewer tokens need to be analyzed.\n","\n","**Better Performance:** In many cases, models perform better when stopwords are removed because the model isn't distracted by words that don't contribute to the meaning of the text.\n","\n"],"metadata":{"id":"xT2zkO9geCmL"}},{"cell_type":"markdown","source":["**• Perform stopword removal on the tokenized words from the text above.**"],"metadata":{"id":"b1McxETxQcDY"}},{"cell_type":"code","source":["from nltk.corpus import stopwords"],"metadata":{"id":"TZ6Bw8dEQes4","executionInfo":{"status":"ok","timestamp":1738207786896,"user_tz":-345,"elapsed":422,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Get English stopwords\n","stop_words = set(stopwords.words('english'))"],"metadata":{"id":"qHMtBCnLQgPT","executionInfo":{"status":"ok","timestamp":1738207793582,"user_tz":-345,"elapsed":382,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["filtered_sentence = [word for word in word_tokens if word.lower() not in stop_words]\n","print(\"Filtered Sentence (Without Stopwords):\", filtered_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9VzsbilDc0nr","executionInfo":{"status":"ok","timestamp":1738207796939,"user_tz":-345,"elapsed":379,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"8cf344da-8061-49c0-f569-a80f45d1ced3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Filtered Sentence (Without Stopwords): ['Data', 'science', 'interdisciplinary', 'field', 'uses', 'scientific', 'methods', 'processes', ',', 'algorithms', ',', 'systems', 'extract', 'knowledge', 'data', '.']\n"]}]},{"cell_type":"markdown","source":["\n","**• What is stemming? Why is it useful in text preprocessing?**\n","\n","`Stemming` is the process of reducing words to their root or base form (also called the \"stem\"), often by removing prefixes or suffixes.\n","\n","**Usefullness:**\n","\n","1. **Reduces Complexity:** Stemming helps normalize words, reducing the number of unique tokens and thus simplifying the model's task.\n","\n","2. **Improves Matching:** It helps identify different forms of a word (e.g., \"running\", \"runner\", \"ran\") as the same word, allowing the model to generalize better.\n","\n","3. **Text Preprocessing:** It is particularly useful in search engines, document classification, and sentiment analysis, where various forms of a word need to be treated as equivalent.\n","\n"],"metadata":{"id":"EpgFHcSAeo_P"}},{"cell_type":"markdown","source":["**• Apply stemming on the following words: ['improving\", \"processed\", \"arguing\", \"analysis']**"],"metadata":{"id":"HhWPIrYAQypD"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer"],"metadata":{"id":"3j4z8awuQ4T6","executionInfo":{"status":"ok","timestamp":1738207891741,"user_tz":-345,"elapsed":412,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["ps = PorterStemmer()"],"metadata":{"id":"pFxuJ0TVQ6LE","executionInfo":{"status":"ok","timestamp":1738207904235,"user_tz":-345,"elapsed":354,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["words = ['improving', 'processed', 'arguing', 'analysis']\n"],"metadata":{"id":"uMfmRTD2Q7au","executionInfo":{"status":"ok","timestamp":1738207904893,"user_tz":-345,"elapsed":3,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Stemming and printing:\n","for word in words:\n","    stemmed_word = ps.stem(word)\n","    print(f\"{word} : {stemmed_word}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LuAmXXkIfSem","executionInfo":{"status":"ok","timestamp":1738208175318,"user_tz":-345,"elapsed":481,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"0ff0add6-c1da-4b3e-a50c-e60f2f578d0c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["improving : improv\n","processed : process\n","arguing : argu\n","analysis : analysi\n"]}]},{"cell_type":"markdown","source":["# **2. Part-of-Speech (POS) Tagging:**\n","**• What is part-of-speech (POS) tagging? How does it help in understanding text?**\n","\n","`Part-of-Speech (POS) tagging` is the process of assigning specific grammatical categories or \"tags\" to each word in a sentence based on its role or function in that sentence. These categories can include nouns, verbs, adjectives, adverbs, prepositions, and more.\n","\n","For example, the word \"run\" can be tagged as a verb (action) or a noun (an activity), depending on the context.\n","\n","\n"],"metadata":{"id":"U67uVxwIgzaQ"}},{"cell_type":"markdown","source":["**• Perform POS tagging on the sentence: \"Machine learning models require large datasets for accurate predictions.\"**"],"metadata":{"id":"hZrDvdplRGfX"}},{"cell_type":"code","source":["# Download the required resource\n","nltk.download('averaged_perceptron_tagger_eng')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jjb0RnegiBNa","executionInfo":{"status":"ok","timestamp":1738207981288,"user_tz":-345,"elapsed":384,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"ee2327e1-5eed-4b4c-c679-46be7a8f06a8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from nltk import pos_tag\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"CPxLXmtCiwWr","executionInfo":{"status":"ok","timestamp":1738207984723,"user_tz":-345,"elapsed":720,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Tokenize the sentence\n","word_tokens = word_tokenize(\"Machine learning models require large datasets for accurate predictions.\")"],"metadata":{"id":"1M5g7NldRQ10","executionInfo":{"status":"ok","timestamp":1738207992090,"user_tz":-345,"elapsed":488,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# POS Tagging\n","pos_tags = pos_tag(word_tokens)\n","print(\"POS Tags:\", pos_tags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ECmiS9O5hE3L","executionInfo":{"status":"ok","timestamp":1738207995943,"user_tz":-345,"elapsed":376,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"0621ad1b-eef0-4ed4-b168-d1bcc5efab6a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["POS Tags: [('Machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('require', 'VBP'), ('large', 'JJ'), ('datasets', 'NNS'), ('for', 'IN'), ('accurate', 'JJ'), ('predictions', 'NNS'), ('.', '.')]\n"]}]},{"cell_type":"markdown","source":["Explanation of POS Tags:\n","\n","1. NN: Singular Noun (e.g., \"machine\")\n","2. NNS: Plural Noun (e.g., \"models\", \"datasets\", \"predictions\")\n","3. VB: Base form of a verb (e.g., \"require\")\n","4. JJ: Adjective (e.g., \"large\", \"accurate\")\n","5. IN: Preposition (e.g., \"for\")"],"metadata":{"id":"t7yoW9B0i429"}},{"cell_type":"markdown","source":["**• Explain the significance of POS tags like MN, v8, and 32 in text analysis.**\n","\n","**1. MN:**\n","  \n","  Significance: In text analysis, recognizing specific noun phrases like monetary amounts (e.g., \"100 dollars\") is crucial for tasks such as financial document processing or automatic summarization.\n","\n","**2. v8:**\n","\n","  Significance: Identifying verb tenses is important for syntactic parsing and sentiment analysis, where the time of the action (past, present, or future) impacts the overall meaning of a sentence or the sentiment conveyed.\n","\n","**3. 32:**\n","\n","  Significance: Extracting numbers and understanding their context is key in tasks like information retrieval, question answering, and document classification, where numeric data is essential."],"metadata":{"id":"zF_vIaKBjPX8"}},{"cell_type":"markdown","source":["# **3. Named Entity Recognition (NER):**\n","**• What is Named Entity Recognition (NER)? List some common types of entities identified by NER models.**\n","\n","`Named Entity Recognition (NER)` is an NLP technique used to identify and classify named entities in text into predefined categories such as names of people, organizations, locations, dates, and more.  \n","\n","**Common types of entities identified by NER models:**\n","1. **Person (PER):** Names of individuals (e.g., \"Elon Musk\").  \n","2. **Organization (ORG):** Companies, institutions, etc. (e.g., \"Google\").  \n","3. **Location (LOC):** Cities, countries, landmarks (e.g., \"Kathmandu\").  \n","4. **Date**/Time (DATE/TIME): Specific dates and times (e.g., \"January 30, 2025\").  \n","5. **Money (MONEY):** Currency amounts (e.g., \"$100\").  \n","6. **Percent (PERCENT):** Percentages (e.g., \"50%\").  \n","7. **GPE (Geopolitical Entity):** Countries, states, cities (e.g., \"Nepal\").  \n","8. **Product (PRODUCT):** Named products (e.g., \"iPhone 15\").  \n","\n","\n","\n","\n"],"metadata":{"id":"2W_L3qMxDVAG"}},{"cell_type":"markdown","source":["**• Perform NER on the following sentence: \"Google is planning to open a new office in New York next year.\"**"],"metadata":{"id":"awlVgau1ScKP"}},{"cell_type":"code","source":["import spacy"],"metadata":{"id":"H0xf91dzSyEp","executionInfo":{"status":"ok","timestamp":1738208400147,"user_tz":-345,"elapsed":9081,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Load English model\n","nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"RXoBOlJ0SzbT","executionInfo":{"status":"ok","timestamp":1738208406809,"user_tz":-345,"elapsed":1728,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Applying NER - Google is planning to open a new office in New York next year.\n","doc = nlp('Google is planning to open a new office in New York next year.')\n","for ent in doc.ents:\n","    print(ent.text, \":\", ent.label_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNt_05bXGe66","executionInfo":{"status":"ok","timestamp":1738208436108,"user_tz":-345,"elapsed":416,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"70eb1300-8273-4a29-f29f-f027897f1ad0"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Google : ORG\n","New York : GPE\n","next year : DATE\n"]}]},{"cell_type":"markdown","source":["**• Identify the entities and explain their labels.**\n","\n","Identified entities and their explanations:\n","\n","1. **Google: ORG (Organization):**  \n","   - Represents a company, institution, or organization.  \n","   - Example: Google, Microsoft, OpenAI.  \n","\n","2. **New York: GPE (Geopolitical Entity):**\n","   - Refers to geographical locations such as cities, states, or countries.  \n","   - Example: New York, Nepal, France.  \n","\n","3. **next year: DATE (Date/Time):**\n","   - Represents a time-related entity, including specific dates, years, or relative time expressions.  \n","   - Example: 2025, January 30, next month.  "],"metadata":{"id":"97xy8M4QGdxY"}},{"cell_type":"markdown","source":["# **4. Sentiment Analvsis:**\n","**• What is sentiment analysis? How is it useful in understanding text data?**\n","\n","`Sentiment Analysis` is an NLP technique used to determine the emotional tone of a given text. It classifies text into categories such as positive, negative, or neutral based on the sentiment expressed.  \n","\n","**Usefulness in understanding text data:**\n","1. **Customer Feedback Analysis:** Helps businesses analyze reviews and improve services.  \n","2. **Brand Monitoring:** Tracks public perception of a company or product.  \n","3. **Market Research:** Identifies trends and consumer opinions.  \n","4. **Social Media Analysis:** Detects public sentiment towards events, policies, or individuals.  \n","5. **Automated Support Systems:** Enhances chatbot responses by understanding user emotions.  \n","\n","\n"],"metadata":{"id":"vXySQmp1HSye"}},{"cell_type":"markdown","source":["**• Perform sentiment analysis on the following text: \"The project outcome was highly satisfying, and the team did an excellent job.\" \"The service was terrible, and the support was unresponsive.\"**"],"metadata":{"id":"k4y7W7rzTjSZ"}},{"cell_type":"code","source":["from transformers import pipeline"],"metadata":{"id":"ABCjHKAATnj5","executionInfo":{"status":"ok","timestamp":1738208626798,"user_tz":-345,"elapsed":13018,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Initialize sentiment analysis model\n","classifier = pipeline(\"sentiment-analysis\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ohqErIZTowE","executionInfo":{"status":"ok","timestamp":1738208629387,"user_tz":-345,"elapsed":2597,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"26ee6e95-e2a5-4ce7-ecc0-2d69502da8c5"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Device set to use cpu\n"]}]},{"cell_type":"code","source":["# Example text for sentiment analysis\n","sentiment = classifier(\"The project outcome was highly satisfying, and the team did an excellent job.\")\n","print(sentiment)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FPtc22rIJEJl","executionInfo":{"status":"ok","timestamp":1738208632295,"user_tz":-345,"elapsed":389,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"81864897-0ca6-4858-8f3c-aac83313ce7f"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'label': 'POSITIVE', 'score': 0.9998728036880493}]\n"]}]},{"cell_type":"code","source":["# Example text for sentiment analysis\n","sentiment = classifier(\"The service was terrible, and the support was unresponsive.\")\n","print(sentiment)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uhbsyrJwJr2O","executionInfo":{"status":"ok","timestamp":1738208638064,"user_tz":-345,"elapsed":390,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"7ee56ad3-d3d1-4025-9499-2bf55d59285b"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'label': 'NEGATIVE', 'score': 0.9995108842849731}]\n"]}]},{"cell_type":"markdown","source":["**• Explain the sentiment results and their interpretation.**\n","*   The first statement conveys praise and satisfaction, making it strongly positive.\n","*   The second statement reflects frustration and disappointment, making it strongly negative.\n","\n"],"metadata":{"id":"TTBLrDB4I_6L"}},{"cell_type":"markdown","source":["# **5. Text Generation:**\n","**• What is text generation in NLP? Provide real-world applications of text generation models.**\n","\n","`Text generation` in NLP is the process of generating coherent, meaningful text based on a given input or prompt. It uses models like GPT to predict the next word or sequence of words.\n","\n","**Real-world applications:**\n","- **Chatbots/Assistants:** Customer support, virtual assistants.\n","- **Content Creation:** Automated writing for blogs, news articles.\n","- **Text Summarization:** Creating concise summaries of lengthy texts.\n","- **Creative Writing:** Generating stories, poetry, or dialogue.\n","- **Translation:** Converting text between languages.\n","\n","\n"],"metadata":{"id":"1CyI9UsTKi3L"}},{"cell_type":"markdown","source":["**• Use a pre-trained language model to generate text for the prompt: \"Artificial intelligence is revolutionizing\"**"],"metadata":{"id":"QySCG4MLT8Yw"}},{"cell_type":"code","source":["generator = pipeline(\"text-generation\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VZj8oiIUEfU","executionInfo":{"status":"ok","timestamp":1738208730403,"user_tz":-345,"elapsed":3193,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"ed13dd39-c45c-45dd-9b4b-d6270e89325c"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Device set to use cpu\n"]}]},{"cell_type":"code","source":["# Generate text\n","generated_text = generator(\"Artificial intelligence is revolutionizing\", max_length=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIY_eafhMKa1","executionInfo":{"status":"ok","timestamp":1738208740943,"user_tz":-345,"elapsed":8347,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"2ec64d3f-5c73-4099-b9bd-9b10b8e3fa3b"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}]},{"cell_type":"code","source":["print(\"Generated Text:\", generated_text[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cx5KC58LMujU","executionInfo":{"status":"ok","timestamp":1738208744058,"user_tz":-345,"elapsed":385,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"2e3f0e2b-60de-4ab0-8362-745c0d556aec"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Text: Artificial intelligence is revolutionizing the way we live. To paraphrase the \"paleo\" philosophy, AI is coming into being in every form, and I am committed to making it the most powerful it can possibly be, until at the very most important milestone it can make an impact. I am talking about AI in terms of power—that is to say, how much, how widely, and how hard people can do something. This is why I am starting the Future of Our Species Program\n"]}]},{"cell_type":"markdown","source":["**• Discuss how text generation models like GPT work.**\n","\n","Text generation models like GPT (Generative Pretrained Transformer) work by predicting the next word or sequence of words based on a given input. Here's how they function:\n","\n","1. **Pretraining:** The model is trained on large datasets of text to learn patterns, grammar, and context.\n","2. **Architecture:** GPT uses a Transformer architecture with attention mechanisms, which allow the model to focus on relevant words in a sentence and capture long-range dependencies.\n","3. **Input Processing:** When a prompt is given, the model tokenizes the input into smaller units (words or subwords).\n","4. **Prediction:** GPT predicts the next word by calculating probabilities for each possible next word, based on context learned during pretraining.\n","5. **Output:** The model generates a coherent continuation of the input prompt by selecting the most probable words."],"metadata":{"id":"m5WVFiFvMJf8"}},{"cell_type":"markdown","source":["# **6. Text Summarization:**\n","**• What is text summarization? Differentiate between extractive and abstractive summarization techniques.**\n","\n","`Text Summarization` is the process of reducing a large piece of text to a shorter version while retaining key information and meaning.\n","\n","**Types of Summarization:**\n","\n","**1. Extractive Summarization:**\n","   - **Method:** Selects and extracts sentences directly from the source text to form a summary.\n","   - **Strength:** Keeps the original wording intact.\n","   - **Limitation:** May lack coherence and flow between selected sentences.\n","   \n","**2. Abstractive Summarization:**\n","   - **Method:** Generates new sentences that convey the core ideas of the original text.\n","   - **Strength:** Produces more coherent and fluent summaries.\n","   - **Limitation:** May introduce errors or lose important details due to rephrasing.\n","\n","\n"],"metadata":{"id":"S2c3lfwDNb_x"}},{"cell_type":"markdown","source":["**• Summarize the following text into 2-3 sentences: \"Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. Al is being applied in a wide range of industries, from healthcare to finance, and has the potential to improve efficiency and decision-making.\"**"],"metadata":{"id":"dord2HezUioW"}},{"cell_type":"code","source":["summarizer = pipeline(\"summarization\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIfq5AidUt6W","executionInfo":{"status":"ok","timestamp":1738208905778,"user_tz":-345,"elapsed":8800,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"2f9fb8f3-7fc4-410c-e0ef-e67f38887111"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Device set to use cpu\n"]}]},{"cell_type":"code","source":["# Example text for summarization\n","long_text = \"\"\"\n","Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. Al is being applied in a wide range of industries, from healthcare to finance, and has the potential to improve efficiency and decision-making.\n","\"\"\""],"metadata":{"id":"CiRgJ3SSOQqL","executionInfo":{"status":"ok","timestamp":1738208908782,"user_tz":-345,"elapsed":720,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Generate summary\n","summary = summarizer(long_text, max_length=50, min_length=25)\n","print(\"Summary:\", summary[0]['summary_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xqxYy1xDOoZF","executionInfo":{"status":"ok","timestamp":1738208927784,"user_tz":-345,"elapsed":14303,"user":{"displayName":"Tashi Lama","userId":"07606106385661302389"}},"outputId":"3a7a81d8-51bb-4f31-c4bf-a28eb7bb65d6"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Summary:  Artificial intelligence is being applied in a wide range of industries, from healthcare to finance . It has the potential to improve efficiency and decision-making .\n"]}]},{"cell_type":"markdown","source":["**• Discuss the importance of summarization in NLP.**\n","\n","**Importance of Summarization in NLP:**\n","\n","**1. Information Overload:** Summarization helps condense large volumes of text, making it easier to digest and extract important information quickly.\n","  \n","**2. Efficiency:** Saves time by providing concise versions of lengthy documents, making it ideal for busy professionals or researchers.\n","\n","**3. Search & Retrieval:** Enhances the relevance of search results by offering summarized content, improving user experience.\n","\n","**4. Content Creation:** Automates the creation of summaries for news articles, reports, and social media, aiding content generation.\n","\n","**5. Data Compression:** Reduces storage requirements by creating more compact representations of information, especially useful for large datasets.\n","\n","**6. Improved Understanding:** Helps users quickly grasp the essence of texts, making it easier to identify key points, especially in technical or academic content."],"metadata":{"id":"G4oeHhl6OS-d"}}]}